## Jigsaw Unintended Bias in Toxicity Classification

## Description

This project is a part of the Machine Learning Course provided by Upskill Income Sharing Agreement program with Intelligent Machines. An already ended competition dataset has been selected as this project where different deep learning models were benchmarked. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. Here it is mentionable that **this is a Kernels-only competition** [(know more)](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/Kernels-Requirements).

## Important Links

* [Dataset](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data)
* [fastText](https://www.kaggle.com/yekenot/fasttext-crawl-300d-2m)
* [GloVe](https://www.kaggle.com/takuok/glove840b300dtxt)
* [word2vec](https://www.kaggle.com/umbertogriffo/googles-trained-word2vec-model-in-python)

## Getting Started

The main challenge of this project is the gigantic amount of train data. The main starting point should be data exploration, data cleaning, dealing with the null values, feature engineering. This is a binary classification problem & highly imbalanced dataset.

## Dependencies

* **Programming Language:** Python
* **Libraries:** NumPy, Pandas, Matplotlib, Seaborn, Keras, TensorFlow
* **Environment:** Kaggle Notebook
