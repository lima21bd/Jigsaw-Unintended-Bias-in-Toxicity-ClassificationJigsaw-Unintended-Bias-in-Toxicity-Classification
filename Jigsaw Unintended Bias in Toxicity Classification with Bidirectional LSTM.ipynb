{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Discussion\n",
    "\n",
    "In this problem I have to develop machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. This is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies\n",
    "\n",
    "* Numpy = Performs a number of mathematical operations on arrays \n",
    "* Pandas = Imports data from csv, Data manipulation operation\n",
    "* Keras = Reduces the used amount of memory resources, offers consistent & simple APIs\n",
    "* TensorFlow = Tensorflow is working in the backend for the tensor manipulation\n",
    "* Warning = This module is used to ignore warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the following function preprocess() is to clean and remove any punctuation marks from the common_text column. Replacing all punctation with the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \n",
    "    punctuation = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    \n",
    "    def clean_special_chars(text, punct):\n",
    "        \n",
    "        for p in punctuation:\n",
    "            \n",
    "            text = text.replace(p, ' ')\n",
    "            \n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punctuation))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocess(train['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have preprocess the commom_text column from the training set and saved it in another dataframe x_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Word Embedding \n",
    "\n",
    "*Word Embedding: Renders a way to use an efficient, dense representation in which similar words have a similar encoding.*\n",
    "\n",
    "'In this project I have tried three non-contextualized word embeddings such as word2vec, GloVe, fastText. fastText came up with a victorious result.'\n",
    "\n",
    "*FastText Crawl 300-dimensional pretrained FastText English word vectors released by Facebook.*\n",
    "\n",
    "Good sites of fastText are as follows:\n",
    "\n",
    "* Captures the meaning of shorter words\n",
    "* Allows the embeddings to understand suffixes and prefixes\n",
    "* fastText works well with rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_files = ['../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', '../input/glove840b300dtxt/glove.840B.300d.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Vectors\n",
    "\n",
    "The following three function extract vectors from word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    \n",
    "    return word, np.asarray(arr, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    \n",
    "    with open(path) as f:\n",
    "        \n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, path):\n",
    "    \n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        \n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "            \n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Type\n",
    "\n",
    "identity_columns which allow us to categorize the type of comment it is and according which we will allocate weight to these columns in our next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', \n",
    "                    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Allocating weights to the identity_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "weights = np.ones((len(x_train),)) / 4\n",
    "\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "\n",
    "loss_weight = 1.0 / weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = preprocess(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Length of each comment is 220.\n",
    "\n",
    "MAX_LEN = 220 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in embedding_files], axis =- 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 1\n",
    "BATCH_SIZE = 100\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model Architecture*\n",
    "\n",
    "1. Input\n",
    "2. Word Embedding\n",
    "3. Dropout \n",
    "4. Bidirectional CuDNN LSTM\n",
    "5. Bidirectional CuDNN LSTM\n",
    "6. Concatenation of GlobalMaxPooling1D & GlobalAveragePooling1D\n",
    "7. Two hidden layer with activation function relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets, loss_weight):\n",
    "    \n",
    "    words = Input(shape = (MAX_LEN,))\n",
    "    word_embedding = Embedding(*embedding_matrix.shape, weights = [embedding_matrix], trainable = False)(words)\n",
    "    dropout = SpatialDropout1D(0.3)(word_embedding)\n",
    "    blstm_1 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences = True))(dropout)\n",
    "    blstm_2 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences = True))(blstm_2)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(blstm_2),\n",
    "        GlobalAveragePooling1D()(blstm_2),\n",
    "    ])\n",
    "    \n",
    "    hidden_1 = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation = 'relu')(hidden)])\n",
    "    hidden_2 = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation = 'relu')(hidden_1)])\n",
    "    \n",
    "    result = Dense(1, activation = 'sigmoid')(hidden_2)\n",
    "    aux_result = Dense(num_aux_targets, activation = 'sigmoid')(hidden_2)\n",
    "    \n",
    "    model = Model(inputs = words, outputs = [result, aux_result])\n",
    "    model.compile(loss = [custom_loss, 'binary_crossentropy'], loss_weights = [loss_weight, 1.0], optimizer = 'adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Reduction\n",
    "\n",
    "Using temporary file to reduce memory. \n",
    "\n",
    "Pickle: It's the process of converting a Python object into a byte stream to store it in a file or database, maintain program state across sessions.\n",
    "\n",
    "gc: gc exposes the underlying memory management mechanism of Python, the automatic garbage collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gc\n",
    "\n",
    "with open('temporary.pickle', mode = 'wb') as f:\n",
    "    \n",
    "    pickle.dump(x_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del identity_columns, weights, tokenizer, train, test, x_test #unnecessary data frames\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_predictions = []\n",
    "weights = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_idx in range(NUM_MODELS):\n",
    "    \n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n",
    "    \n",
    "    for global_epoch in range(EPOCHS):\n",
    "        \n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size = BATCH_SIZE,\n",
    "            epochs = 1,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "        with open('temporary.pickle', mode='rb') as f:\n",
    "            x_test = pickle.load(f)\n",
    "            \n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size = 1024)[0].flatten())\n",
    "        \n",
    "        del x_test\n",
    "        gc.collect()\n",
    "        \n",
    "        weights.append(2 ** global_epoch)\n",
    "        \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.average(checkpoint_predictions, weights = weights, axis = 0)\n",
    "\n",
    "df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
    "df_submit.prediction = predictions\n",
    "\n",
    "df_submit.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
